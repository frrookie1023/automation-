{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "def text_to_csv(input_filename, output_filename): #convert txt file to csv \n",
    "    with open(input_filename, 'r') as txt: #open the file\n",
    "        lines = txt.readlines() #read all the lines \n",
    "    with open(output_filename, 'w') as csv_file: #set up the csv file \n",
    "        for line in lines:\n",
    "            fields = line.strip().split()  # split the line\n",
    "            csv_line = ','.join(fields)    # Join fields with commas for CSV\n",
    "            csv_file.write(csv_line + '\\n')\n",
    "def census_geographic(driver, unique):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "    select.click()\n",
    "    time.sleep(3)\n",
    "    year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[2]/div[1]/div/div')\n",
    "    year_5.click()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "    geo.click()\n",
    "    wait = WebDriverWait(driver,10)\n",
    "    county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "    county.click()\n",
    "    time.sleep(5)\n",
    "    Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "    Clicklist.click()\n",
    "    cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "    cancel.click()\n",
    "    wait = WebDriverWait(driver,15)\n",
    "    download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "    download.click()\n",
    "    time.sleep(3)\n",
    "    year_2020= driver.find_element(By.ID, 'Table: S0701 Vintage: 2020')\n",
    "    year_2020.click()\n",
    "    time.sleep(1)\n",
    "    year_2019= driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div/div/div[2]/div[2]/div[2]/div/div/div[3]/div/div[5]/div/div')\n",
    "    year_2019.click()\n",
    "    time.sleep(1)\n",
    "    elements_2018= driver.find_elements(By.ID, 'Table: S0701 Vintage: 2018') #there are two same ID \n",
    "    if len(elements_2018) >= 2:\n",
    "        for element in elements_2018:\n",
    "            if element == elements_2018[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2017 = driver.find_elements(By.ID, 'Table: S0701 Vintage: 2017') #there are two same IDs\n",
    "    if len(elements_2017) >= 2:\n",
    "        for Element in elements_2017:\n",
    "            if Element == elements_2017[1]:\n",
    "                Element.click()\n",
    "                break\n",
    "    time.sleep(1)            \n",
    "    download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "    download_csv.click()\n",
    "    time.sleep(10)\n",
    "    attempts = 0\n",
    "    max_attempts = 5  # Maximum number of attempts \n",
    "    time_break = 10\n",
    "    while attempts < max_attempts: #trying to do the mechanism \n",
    "        files = os.listdir(download_path)\n",
    "        zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "        if zipfiles:\n",
    "            for zip_file in zipfiles:\n",
    "                zip_file_path = os.path.join(download_path, zip_file)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(download_path)\n",
    "                os.remove(zip_file_path)\n",
    "            break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "        else:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "                time.sleep(time_break)\n",
    "            else:\n",
    "                print(\"No ZIP files found after maximum attempts.\")\n",
    "    time.sleep(10)\n",
    "    years = range(2017, 2021)\n",
    "    for year in years:\n",
    "        csv_file = f'ACSST5Y{year}.S0701-Data.csv'\n",
    "        csv_path = os.path.join(download_path, csv_file)\n",
    "        table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "        table.drop(0, axis=0, inplace=True)\n",
    "        table.columns = table.iloc[0]\n",
    "        table = table[1:]\n",
    "        columns_keyword = []\n",
    "        for column in table.columns:\n",
    "            if isinstance(column, str) and (\"!!Moved\" in column or \"Geo\" in column) and (\"Annotation\" not in column and \"Margin\" not in column):\n",
    "                columns_keyword.append(column)  \n",
    "        selected_columns = table[columns_keyword]\n",
    "        selected_columns.to_csv(f'{year}.csv', index=False)\n",
    "def census_demographics(driver,unique):\n",
    "    driver.get(unique)\n",
    "    time.sleep(5)\n",
    "    select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "    select.click()\n",
    "    time.sleep(3)\n",
    "    year_5 = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "    year_5.click()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "    geo.click()\n",
    "    wait = WebDriverWait(driver,20)\n",
    "    county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "    county.click()\n",
    "    time.sleep(5)\n",
    "    Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "    Clicklist.click()\n",
    "    cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "    cancel.click()\n",
    "    wait = WebDriverWait(driver,15)\n",
    "    download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "    download.click()\n",
    "    time.sleep(3)\n",
    "    year_2020= driver.find_element(By.ID, 'Table: DP05 Vintage: 2020')\n",
    "    year_2020.click()\n",
    "    time.sleep(1)\n",
    "    year_2019= driver.find_elements(By.ID, 'Table: DP05 Vintage: 2019')\n",
    "    if len(year_2019) >= 2:\n",
    "        for element in year_2019:\n",
    "            if element == year_2019[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2018= driver.find_elements(By.ID, 'Table: DP05 Vintage: 2018') #there are two same ID \n",
    "    if len(elements_2018) >= 2:\n",
    "        for element in elements_2018:\n",
    "            if element == elements_2018[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2017 = driver.find_elements(By.ID, 'Table: DP05 Vintage: 2017') #there are two same IDs\n",
    "    if len(elements_2017) >= 2:\n",
    "        for Element in elements_2017:\n",
    "            if Element == elements_2017[1]:\n",
    "                Element.click()\n",
    "                break\n",
    "    time.sleep(1)            \n",
    "    download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "    download_csv.click()\n",
    "    time.sleep(20)\n",
    "    attempts = 0\n",
    "    max_attempts = 5  # Maximum number of attempts \n",
    "    time_break = 10\n",
    "    while attempts < max_attempts: #trying to do the mechanism \n",
    "        files = os.listdir(download_path)\n",
    "        zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "        if zipfiles:\n",
    "            for zip_file in zipfiles:\n",
    "                zip_file_path = os.path.join(download_path, zip_file)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(download_path)\n",
    "                os.remove(zip_file_path)\n",
    "            break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "        else:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "                time.sleep(time_break)\n",
    "            else:\n",
    "                print(\"No ZIP files found after maximum attempts.\")\n",
    "def tamu_umr_report(driver,unique):\n",
    "    driver.get(unique) \n",
    "    download_csv = driver.find_element(By.XPATH, '//*[@id=\"genesis-content\"]/article/div/p[2]/a[1]')\n",
    "    download_csv.click()\n",
    "    time.sleep(10)\n",
    "    excelfile = 'complete-data-2021-umr-by-tti.xlsx'\n",
    "    excel_extracted = pd.read_excel(excelfile)\n",
    "    excel_extracted\n",
    "    combined_labels = excel_extracted.iloc[:4].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=0) #combine first 4 row as one label \n",
    "    excel_extracted = excel_extracted.iloc[4:] #replace first 4 row name\n",
    "    excel_extracted.loc[-1] = combined_labels\n",
    "    excel_extracted.index = excel_extracted.index + 1\n",
    "    excel_extracted.sort_index(inplace=True)\n",
    "    excel_extracted.rename(columns={'Unnamed: 23': 'Annual Hours of Delay'}, inplace=True) #renamed column name \n",
    "    columns = ['Area Group', 'Urban Area', 'Primary','Population','Year','Arterial Street','Annual Hours of Delay']\n",
    "    selected = excel_extracted[columns]\n",
    "    csv_file_path = 'umr.csv'\n",
    "    selected.to_csv(csv_file_path, index=False)\n",
    "    os.remove('complete-data-2021-umr-by-tti.xlsx')\n",
    "def census_data(driver,unique):\n",
    "    driver.get(unique) \n",
    "    download_csv=driver.find_element(By.ID,'anch_33')\n",
    "    download_csv.click()\n",
    "    time.sleep(10)\n",
    "    table = pd.read_excel('bfs_county_apps_annual.xlsx', header= None)\n",
    "    table = table.iloc[2:]\n",
    "    selected = table\n",
    "    csv_file_path = 'Business Applications by County.csv'\n",
    "    selected.to_csv(csv_file_path, index=False)\n",
    "    os.remove('bfs_county_apps_annual.xlsx')\n",
    "def census_survey(driver,unique):\n",
    "    driver.get(unique)\n",
    "    time.sleep(5)\n",
    "    year_2021 = driver.find_element(By.XPATH, '//*[@id=\"listArticlesContainer_List_1222676053\"]/div[2]/a[1]')\n",
    "    year_2021.click()\n",
    "    time.sleep(5)\n",
    "    county_2021 = driver.find_element(By.XPATH, '//*[@id=\"listArticlesContainer_List_309820028\"]/div[2]/div[2]/div/span/a')\n",
    "    county_2021.click()\n",
    "    attempts = 0\n",
    "    max_attempts = 5  # Maximum number of attempts \n",
    "    time_break = 10\n",
    "    while attempts < max_attempts: #trying to do the mechanism \n",
    "        files = os.listdir(download_path)\n",
    "        zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "        if zipfiles:\n",
    "            for zip_file in zipfiles:\n",
    "                zip_file_path = os.path.join(download_path, zip_file)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(download_path)\n",
    "                os.remove(zip_file_path)\n",
    "            break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "        else:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "                time.sleep(time_break)\n",
    "            else:\n",
    "                print(\"No ZIP files found after maximum attempts.\")\n",
    "    time.sleep(10)\n",
    "    input_text_file = 'cbp21co.txt'\n",
    "    output_csv_file = 'cbp21co.csv'\n",
    "    text_to_csv(input_text_file, output_csv_file)\n",
    "def cnbc_business(driver,unique):\n",
    "    current_year = datetime.datetime.now().year  #get the current year\n",
    "    for year in range(2007, current_year + 1): #we make for loop for each year \n",
    "        filtered_columns = [] #we define the empty filtered column \n",
    "        if year in [2009, 2013]: #because 2009 and 2013 overall ranking have unique html comparing to other \n",
    "            if year == 2009:\n",
    "                url = \"https://www.cnbc.com/id/100000992\"\n",
    "            elif year == 2013: #create additional condition\n",
    "                url = \"https://www.cnbc.com/id/100824779\"\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "        else: #other years \n",
    "            if year >= 2014:\n",
    "                for month in range(6,8):\n",
    "                    for day in range(10,24):\n",
    "                        url = f\"https://www.cnbc.com/{year}/{month}/{day}/americas-top-states-for-business.html\"\n",
    "                        response = requests.get(url)\n",
    "                        if response.status_code == 200: #if that url is exisiting, code is 200\n",
    "                            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                            table = soup.find('table')\n",
    "                            break\n",
    "                    else:\n",
    "                        continue #creating iteration for checking the html \n",
    "                    break \n",
    "            else:\n",
    "                url = f\"https://www.cnbc.com/top-states-{year}-overall-rankings/\" #the url linkfor overall rankings\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                table = soup.find('table')\n",
    "        if table:\n",
    "            column_names = []\n",
    "            data_rows = []\n",
    "            for i, tr in enumerate(table.find_all('tr')):\n",
    "                if i == 0:  # Header row\n",
    "                    th_tags = tr.find_all(['th', 'td'])\n",
    "                    column_names = [th.text.strip() for th in th_tags if th.text.strip()]\n",
    "                else:\n",
    "                    row = [td.text.strip() for td in tr.find_all('td')]\n",
    "                    if row:\n",
    "                        data_rows.append(row)\n",
    "       \n",
    "            overall_labels = [\"Overall Rank\", \"Overall\",\"Rank\",\"Overall Ranking\"] #multiple labels\n",
    "            costofbusiness_labels = [\"Cost of Business\", \"Cost of Doing Business\"]\n",
    "            business_columns = [\"Business Friendliness\",\"Business Friendly\"]\n",
    "            Infra = [\"Infrastructure\",\"Transportation\",\"Infrastructure & Transp.\",\"Infrastructure &amp; Transp.\"] #does include transportation??\n",
    "            State = [\"State\"]\n",
    "            table_extract = pd.DataFrame(data_rows, columns=column_names)\n",
    "            table_extract.rename(columns=dict(zip(overall_labels, ['Overall Ranking']*len(overall_labels))), inplace=True)  # Rename into standard form\n",
    "            table_extract.rename(columns=dict(zip(costofbusiness_labels, ['Cost of Doing Business']*len(costofbusiness_labels))), inplace=True)  # Rename into standard form\n",
    "            table_extract.rename(columns=dict(zip(Infra, ['Infrastructure']*len(Infra))), inplace=True)  # Rename into standard form\n",
    "            table_extract.rename(columns=dict(zip(business_columns, ['Business Friendliness']*len(business_columns))), inplace=True)  # Rename into standard form\n",
    "            for label in overall_labels:\n",
    "                if label in table_extract.columns:\n",
    "                    filtered_columns.insert(0, label)\n",
    "                    #rename into standard form \n",
    "                    break\n",
    "            for label in costofbusiness_labels:\n",
    "                if label in table_extract.columns:\n",
    "                    filtered_columns.insert(0, label)\n",
    "                    break\n",
    "            for label in business_columns:\n",
    "                if label in table_extract.columns:\n",
    "                    filtered_columns.insert(0, label)\n",
    "                    break\n",
    "            for label in Infra:\n",
    "                if label in table_extract.columns:\n",
    "                    filtered_columns.insert(0, label)\n",
    "                    break\n",
    "            for label in State:\n",
    "                if label in table_extract.columns:\n",
    "                    filtered_columns.insert(0, label)\n",
    "                    break\n",
    "            filtered_data = table_extract[filtered_columns]\n",
    "            filtered_data.to_csv(f'extracted{year}.csv', index=False, header=True, mode='w')\n",
    "            print(f\" {year} downloaded.\")\n",
    "def census_time(driver,unique):\n",
    "    driver.get(unique)\n",
    "    time.sleep(10)\n",
    "    html = \"https://www2.census.gov/programs-surveys/bds/tables/time-series/bds2020_st_cty.csv\"\n",
    "    driver.get(html)\n",
    "def realtor_research(driver,unique):\n",
    "    url_historical = 'https://econdata.s3-us-west-2.amazonaws.com/Reports/Core/RDC_Inventory_Core_Metrics_County_History.csv'\n",
    "    driver.get(url_historical)\n",
    "    time.sleep(70)\n",
    "    table = pd.read_csv('RDC_Inventory_Core_Metrics_County_History.csv')\n",
    "    table.rename(columns={'month_date_yyyymm': 'year&month'}, inplace=True)\n",
    "    table.rename(columns={'county_name': 'County'}, inplace=True)\n",
    "    table.rename(columns={'county_fips': 'County Federal Information Processing Standard'}, inplace=True)\n",
    "    table.rename(columns={'median_square_feet': 'median listing square feet'}, inplace=True)\n",
    "    columns = ['year&month', 'County', 'County Federal Information Processing Standard','median listing square feet']\n",
    "    selected = table[columns]\n",
    "    csv_file_path = 'realtor.csv'\n",
    "    selected.to_csv(csv_file_path, index=False)\n",
    "    os.remove('RDC_Inventory_Core_Metrics_County_History.csv')\n",
    "def taxfoundation(driver,unique):\n",
    "    url = 'https://taxfoundation.org/2023-state-business-tax-climate-index/' #the url for 2007 overall raking\n",
    "    response = requests.get(url)  #get the webpage\n",
    "#require beautifulsoup to extract html documents\n",
    "    Object = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = Object.find('table') #locate the table in the html\n",
    "    column_names = []\n",
    "    first_row = table.find('tr')\n",
    "    for th in first_row.find_all(['th', 'td']):\n",
    "        column_name = th.text.strip()\n",
    "        if column_name:\n",
    "            column_names.append(column_name)\n",
    "    data_rows = []  #we initalize the data row number \n",
    "    for tr in table.find_all('tr'): #tr represent each number \n",
    "        row = [td.text.strip() for td in tr.find_all('td')] #we finds all the row elements within the current row element and create a new list containing the text content of each row element in the row\n",
    "        if row: #use to check whether there is empty row \n",
    "            data_rows.append(row) #if the row is not empty, then we show it in the data_row\n",
    "    table_tax = pd.DataFrame(data_rows, columns=column_names)\n",
    "    table_tax.to_csv('table_tax.csv', index=False)\n",
    "def census_construction(driver,unique):\n",
    "    base = 'https://www2.census.gov/econ/bps/County/'\n",
    "    driver.get(base)\n",
    "    time.sleep(5)\n",
    "    def process_txt_content(txt_content, output_csv_file):\n",
    "        lines = txt_content.split('\\n')\n",
    "        with open(output_csv_file, 'w') as csv_file:\n",
    "            for line in lines:\n",
    "                fields = line.strip().split()  # Adjust if needed\n",
    "                csv_line = ','.join(fields)\n",
    "                csv_file.write(csv_line + '\\n')\n",
    "    fourth_digit = 1\n",
    "    third_digit = 0   #from year 2018 \n",
    "    stop_loop = False #we define the stop loop for the entire iteration \n",
    "    generated_filenames = set()\n",
    "    while fourth_digit < 10 and not stop_loop:\n",
    "        for last_two_digits in range(1, 14):  # The last two digits are changing from 1 to 13\n",
    "            if fourth_digit == 2 and third_digit == 3 and last_two_digits == 7:\n",
    "                if file_type == 'c':\n",
    "                    stop_loop = True\n",
    "                    driver.quit()\n",
    "                    break \n",
    "            if last_two_digits > 12:\n",
    "                third_digit += 1\n",
    "                if third_digit == 10:\n",
    "                    fourth_digit += 1\n",
    "                    third_digit = 0\n",
    "                adjusted_last_digits = last_two_digits - 12 \n",
    "                adjusted = True\n",
    "            else:\n",
    "                adjusted_last_digits = last_two_digits\n",
    "                adjusted = False  \n",
    "            for file_type in ['c', 'y']:\n",
    "                if fourth_digit == 2 and third_digit == 3 and last_two_digits == 7 and file_type == 'c':\n",
    "                    stop_loop = True\n",
    "                    driver.quit()\n",
    "                    break\n",
    "                else:\n",
    "                    file_name = f\"co{str(fourth_digit)}{str(third_digit)}{str(adjusted_last_digits).zfill(2)}{file_type}.txt\"\n",
    "                    if file_name in generated_filenames:\n",
    "                        continue\n",
    "                    generated_filenames.add(file_name) \n",
    "                    full_url = base + file_name\n",
    "                    driver.get(full_url)\n",
    "                    driver.refresh()\n",
    "                    time.sleep(2)\n",
    "                    response = requests.get(full_url)\n",
    "                    if response.status_code == 200:\n",
    "                        content = response.text\n",
    "                        local_file_path = f\"co{str(fourth_digit)}{str(third_digit)}{str(adjusted_last_digits).zfill(2)}{file_type}.txt\"\n",
    "                        with open(local_file_path, \"w\") as file:\n",
    "                            file.write(content)\n",
    "                        table = pd.read_csv(local_file_path, sep=',')\n",
    "                        table.reset_index(inplace=True)\n",
    "                        table.drop(columns=['FIPS.1', 'Region', 'Unnamed: 6', 'Unnamed: 9', 'Unnamed: 12', 'Unnamed: 15', 'Unnamed: 17', 'Unnamed: 18', '1-unit rep', 'Unnamed: 20', 'Unnamed: 21', '2-units rep', 'Unnamed: 23', 'Unnamed: 24', '3-4 units rep', 'Unnamed: 26', 'Unnamed: 27', ' 5+units rep'], inplace=True)\n",
    "                        table.rename(columns={'Survey': 'State', 'FIPS': 'County', 'Division': 'County Name', 'County': 'total1Unit', '1-unit': 'value1Unit', 'Unnamed: 8': 'total2Unit', '2-units': 'value2Unit', 'Unnamed: 11': 'total34Unit', '3-4 units': 'value34Unit', 'Unnamed: 14': 'total5Unit', '5+ units': 'value5Unit'}, inplace=True)\n",
    "                        table.drop(index=0, inplace=True)\n",
    "                        table['total1Unit'] = table['total1Unit'].astype(float)\n",
    "                        table['total2Unit'] = table['total2Unit'].astype(float)\n",
    "                        table['total34Unit'] = table['total34Unit'].astype(float)\n",
    "                        table['total5Unit'] = table['total5Unit'].astype(float)\n",
    "                        table['value1Unit'] = table['value1Unit'].astype(float)\n",
    "                        table['value2Unit'] = table['value2Unit'].astype(float)\n",
    "                        table['value34Unit'] = table['value34Unit'].astype(float)\n",
    "                        table['value5Unit'] = table['value5Unit'].astype(float)\n",
    "                        table['totalBuildings10'] = table['total1Unit'] + table['total2Unit'] + table['total34Unit'] + table['total5Unit']\n",
    "                        table['totalBuildingValue10'] = table['value1Unit'] + table['value2Unit'] + table['value34Unit'] + table['value5Unit']\n",
    "                        table['GEO_ID'] = \"0500000US\" + table['State'] + table['County']\n",
    "                        formatted_columns = ['State', 'County']  # Add more column names as needed\n",
    "                        for column in formatted_columns:\n",
    "                            if column == 'State':\n",
    "                                table[column] = table[column].apply(lambda x: f\"{int(x):02}\" if x.isdigit() else x)\n",
    "                            elif column == 'County':\n",
    "                                table[column] = table[column].apply(lambda x: f\"{int(x):03}\" if x.isdigit() else x)\n",
    "                        columns = ['index', 'State', 'County Name','County','totalBuildings10','totalBuildingValue10','GEO_ID']\n",
    "                        selected = table[columns]\n",
    "                        csv_file = f\"co{str(fourth_digit)}{str(third_digit)}{str(adjusted_last_digits).zfill(2)}{file_type}.csv\"\n",
    "                        selected.to_csv(csv_file, index=False)\n",
    "                        os.remove(local_file_path)\n",
    "                    else:\n",
    "                        print(\"Failed to download the text file\")    \n",
    "                    if stop_loop:\n",
    "                        break     \n",
    "                if adjusted:\n",
    "                    break\n",
    "def zillow(driver, unique):\n",
    "    county_url = 'https://files.zillowstatic.com/research/public_csvs/zhvi/State_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv?t=1690916965'\n",
    "    driver.get(county_url)\n",
    "    time.sleep(5)\n",
    "def climate_hazard(driver, unique):\n",
    "    url = 'https://hazards.fema.gov/nri/data-resources#csvDownload'\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    download = driver.find_element(By.XPATH, '//*[@id=\"csvDownload\"]/div/div/div[1]/ul/li[1]/a')\n",
    "    download.click()\n",
    "    attempts = 0\n",
    "    max_attempts = 5  # Maximum number of attempts \n",
    "    time_break = 10\n",
    "    while attempts < max_attempts: #trying to do the mechanism \n",
    "        files = os.listdir(download_path)\n",
    "        zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "        if zipfiles:\n",
    "            for zip_file in zipfiles:\n",
    "                zip_file_path = os.path.join(download_path, zip_file)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(download_path)\n",
    "                os.remove(zip_file_path)\n",
    "            break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "        else:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "                time.sleep(time_break)\n",
    "            else:\n",
    "                print(\"No ZIP files found after maximum attempts.\")\n",
    "    files = os.listdir(download_path)\n",
    "    for file in files:\n",
    "        if 'NRI' in file and not file.endswith('.csv'):\n",
    "            os.remove(os.path.join(download_path, file))\n",
    "def Hudgis(driver, unique):\n",
    "    driver.get(url)\n",
    "    time.sleep(20)\n",
    "    download= driver.find_element(By.XPATH, '//*[@id=\"ember51\"]/div/div/div[3]/button')\n",
    "    download.click()\n",
    "    time.sleep(10)\n",
    "    actions = ActionChains(driver)\n",
    "    x_coord = 346 # x-coordinate\n",
    "    y_coord = 357  #  Y-coordinate\n",
    "    actions.move_by_offset(x_coord, y_coord)\n",
    "    actions.click().perform()\n",
    "    time.sleep(120)\n",
    "    \n",
    "def census_DP02(driver, unique):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "    select.click()\n",
    "    time.sleep(3)\n",
    "    year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[4]/div[1]/div/div')\n",
    "    year_5.click()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "    geo.click()\n",
    "    wait = WebDriverWait(driver,10)\n",
    "    county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "    county.click()\n",
    "    time.sleep(5)\n",
    "    Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "    Clicklist.click()\n",
    "    cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "    cancel.click()\n",
    "    wait = WebDriverWait(driver,15)\n",
    "    download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "    download.click()\n",
    "    time.sleep(5)\n",
    "    year_2020= driver.find_element(By.ID, 'Table: DP02 Vintage: 2020')\n",
    "    year_2020.click()\n",
    "    time.sleep(1)\n",
    "    elements_2019= driver.find_elements(By.ID, 'Table: DP02 Vintage: 2019')\n",
    "    if len(elements_2019) >= 2:\n",
    "        for element in elements_2019:\n",
    "            if element == elements_2019[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2018= driver.find_elements(By.ID, 'Table: DP02 Vintage: 2018') #there are two same ID \n",
    "    if len(elements_2018) >= 2:\n",
    "        for element in elements_2018:\n",
    "            if element == elements_2018[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2017 = driver.find_elements(By.ID, 'Table: DP02 Vintage: 2017') #there are two same IDs\n",
    "    if len(elements_2017) >= 2:\n",
    "        for Element in elements_2017:\n",
    "            if Element == elements_2017[1]:\n",
    "                Element.click()\n",
    "                break\n",
    "    time.sleep(1)            \n",
    "    download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "    download_csv.click()\n",
    "    time.sleep(120)\n",
    "    attempts = 0\n",
    "    max_attempts = 100  # Maximum number of attempts \n",
    "    time_break = 10\n",
    "    while attempts < max_attempts: #trying to do the mechanism \n",
    "        files = os.listdir(download_path)\n",
    "        zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "        if zipfiles:\n",
    "            for zip_file in zipfiles:\n",
    "                zip_file_path = os.path.join(download_path, zip_file)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(download_path)\n",
    "                os.remove(zip_file_path)\n",
    "            break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "        else:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "                time.sleep(time_break)\n",
    "            else:\n",
    "                print(\"No ZIP files found after maximum attempts.\")\n",
    "    time.sleep(10)\n",
    "    years = range(2017, 2022)\n",
    "    for year in years:\n",
    "        csv_file = f'ACSDP5Y{year}.DP02-Data.csv' \n",
    "        csv_path = os.path.join(download_path, csv_file)\n",
    "        table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "        table.drop(0, axis=0, inplace=True)\n",
    "        table.columns = table.iloc[0]\n",
    "        table = table[1:]\n",
    "        columns_keyword = []\n",
    "        for column in table.columns:\n",
    "            if isinstance(column, str) and (\"Geo\" in column or \"total\" in column or \"Geo\" in column or \"fam\" in column or \"child\" in column or \"graduate\" in column or \"imm\" in column or \"bachelor\" in column) and (\"Annotation\" not in column and \"Margin\" not in column):\n",
    "                columns_keyword.append(column)  \n",
    "        selected_columns = table[columns_keyword]\n",
    "        selected_columns.to_csv(f'DP02-{year}.csv', index=False)\n",
    "\n",
    "def census_S2301(driver, unique):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "    select.click()\n",
    "    time.sleep(3)\n",
    "    year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[2]/div[1]/div/div')\n",
    "    year_5.click()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "    geo.click()\n",
    "    wait = WebDriverWait(driver,10)\n",
    "    county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "    county.click()\n",
    "    time.sleep(5)\n",
    "    Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "    Clicklist.click()\n",
    "    cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "    cancel.click()\n",
    "    wait = WebDriverWait(driver,15)\n",
    "    download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "    download.click()\n",
    "    time.sleep(5)\n",
    "    year_2020= driver.find_element(By.ID, 'Table: S2301 Vintage: 2020')\n",
    "    year_2020.click()\n",
    "    time.sleep(1)\n",
    "    elements_2019= driver.find_elements(By.ID, 'Table: S2301 Vintage: 2019')\n",
    "    if len(elements_2019) >= 2:\n",
    "        for element in elements_2019:\n",
    "            if element == elements_2019[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2018= driver.find_elements(By.ID, 'Table: S2301 Vintage: 2018') #there are two same ID \n",
    "    if len(elements_2018) >= 2:\n",
    "        for element in elements_2018:\n",
    "            if element == elements_2018[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2017 = driver.find_elements(By.ID, 'Table: S2301 Vintage: 2017') #there are two same IDs\n",
    "    if len(elements_2017) >= 2:\n",
    "        for Element in elements_2017:\n",
    "            if Element == elements_2017[1]:\n",
    "                Element.click()\n",
    "                break\n",
    "    time.sleep(1)            \n",
    "    download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "    download_csv.click()\n",
    "    time.sleep(20)\n",
    "    attempts = 0\n",
    "    max_attempts = 100  # Maximum number of attempts \n",
    "    time_break = 10\n",
    "    while attempts < max_attempts: #trying to do the mechanism \n",
    "        files = os.listdir(download_path)\n",
    "        zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "        if zipfiles:\n",
    "            for zip_file in zipfiles:\n",
    "                zip_file_path = os.path.join(download_path, zip_file)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(download_path)\n",
    "                os.remove(zip_file_path)\n",
    "            break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "        else:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "                time.sleep(time_break)\n",
    "            else:\n",
    "                print(\"No ZIP files found after maximum attempts.\")\n",
    "    time.sleep(10)\n",
    "    years = range(2017, 2022)\n",
    "    for year in years:\n",
    "        csv_file = f'ACSST5Y{year}.S2301-Data.csv' \n",
    "        csv_path = os.path.join(download_path, csv_file)\n",
    "        table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "        table.drop(0, axis=0, inplace=True)\n",
    "        table.columns = table.iloc[0]\n",
    "        table = table[1:]\n",
    "        columns_keyword = []\n",
    "        for column in table.columns:\n",
    "            if isinstance(column, str) and (\"Geo\" in column or \"total\" in column or \"Geo\" in column or \"Employ\" in column or \"Unemploy\" in column) and (\"Annotation\" not in column and \"Margin\" not in column):\n",
    "                columns_keyword.append(column)  \n",
    "        selected_columns = table[columns_keyword]\n",
    "        selected_columns.to_csv(f'S2301-{year}.csv', index=False)\n",
    "        \n",
    "def census_B25104(driver, unique):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "    select.click()\n",
    "    time.sleep(3)\n",
    "    year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[2]/div[1]/div/div')\n",
    "    year_5.click()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "    geo.click()\n",
    "    wait = WebDriverWait(driver,10)\n",
    "    county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "    county.click()\n",
    "    time.sleep(5)\n",
    "    Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "    Clicklist.click()\n",
    "    cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "    cancel.click()\n",
    "    wait = WebDriverWait(driver,15)\n",
    "    download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "    download.click()\n",
    "    time.sleep(5)\n",
    "    year_2020= driver.find_element(By.ID, 'Table: B25104 Vintage: 2020')\n",
    "    year_2020.click()\n",
    "    time.sleep(1)\n",
    "    elements_2019= driver.find_elements(By.ID, 'Table: B25104 Vintage: 2019')\n",
    "    if len(elements_2019) >= 2:\n",
    "        for element in elements_2019:\n",
    "            if element == elements_2019[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2018= driver.find_elements(By.ID, 'Table: B25104 Vintage: 2018') #there are two same ID \n",
    "    if len(elements_2018) >= 2:\n",
    "        for element in elements_2018:\n",
    "            if element == elements_2018[1]:\n",
    "                element.click()\n",
    "                break\n",
    "    time.sleep(1)\n",
    "    elements_2017 = driver.find_elements(By.ID, 'Table: B25104 Vintage: 2017') #there are two same IDs\n",
    "    if len(elements_2017) >= 2:\n",
    "        for Element in elements_2017:\n",
    "            if Element == elements_2017[1]:\n",
    "                Element.click()\n",
    "                break\n",
    "    time.sleep(1)            \n",
    "    download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "    download_csv.click()\n",
    "    time.sleep(20)\n",
    "    attempts = 0\n",
    "    max_attempts = 100  # Maximum number of attempts \n",
    "    time_break = 10\n",
    "    while attempts < max_attempts: #trying to do the mechanism \n",
    "        files = os.listdir(download_path)\n",
    "        zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "        if zipfiles:\n",
    "            for zip_file in zipfiles:\n",
    "                zip_file_path = os.path.join(download_path, zip_file)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(download_path)\n",
    "                os.remove(zip_file_path)\n",
    "            break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "        else:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "                time.sleep(time_break)\n",
    "            else:\n",
    "                print(\"No ZIP files found after maximum attempts.\")\n",
    "    time.sleep(10)\n",
    "    years = range(2017, 2022)\n",
    "    for year in years:\n",
    "        csv_file = f'ACSDT5Y{year}.B25104-Data.csv' \n",
    "        csv_path = os.path.join(download_path, csv_file)\n",
    "        table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "        table.drop(0, axis=0, inplace=True)\n",
    "        table.columns = table.iloc[0]\n",
    "        table = table[1:]\n",
    "        columns_keyword = []\n",
    "        for column in table.columns:\n",
    "            if isinstance(column, str) and (\"Geo\" in column or \"Total\" in column) and (\"Annotation\" not in column and \"Margin\" not in column):\n",
    "                columns_keyword.append(column)  \n",
    "        selected_columns = table[columns_keyword]\n",
    "        selected_columns.to_csv(f'B25104-{year}.csv', index=False)\n",
    "        os.remove(csv_file)\n",
    "        \n",
    "target = {\n",
    "        \"https://data.census.gov/table?q=geographic+mobility&tid=ACSST1Y2021.S0701\": census_geographic,\n",
    "        \"https://data.census.gov/table?q=demographics&tid=ACSDP1Y2021.DP05\":census_demographics,\n",
    "        \"https://mobility.tamu.edu/umr/report/\":tamu_umr_report,\n",
    "        \"https://www.census.gov/econ/bfs/data/county.html\":census_data,\n",
    "        \"https://www.census.gov/programs-surveys/cbp/data/datasets.html\":census_survey,\n",
    "        \"https://www.cnbc.com/2022/07/13/americas-top-states-for-business-2022-the-full-rankings.html\":cnbc_business,\n",
    "        \"https://www.census.gov/data/datasets/time-series/econ/bds/bds-datasets.html\":census_time,\n",
    "        \"https://www.realtor.com/research/data/\":realtor_research,\n",
    "        \"https://taxfoundation.org/2023-state-business-tax-climate-index/\":taxfoundation,\n",
    "        \"https://www.census.gov/construction/bps/index.html\":census_construction,\n",
    "        \"https://www.zillow.com/research/data/\":zillow,\n",
    "        \"https://hazards.fema.gov/nri/map\":climate_hazard,\n",
    "        \"https://hudgis-hud.opendata.arcgis.com/datasets/HUD::location-affordability-index-v-3-1/explore\":Hudgis,\n",
    "        \"https://data.census.gov/table?+q=United+States&g=010XX00US&tid=ACSDP5Y2020.DP02\":census_DP02,\n",
    "        \"https://data.census.gov/table?q=unemployment&g=010XX00US&tid=ACSST1Y2021.S2301\":census_S2301,\n",
    "        \"https://data.census.gov/table?q=monthly+housing+payment&tid=ACSDT1Y2021.B25104\":census_B25104\n",
    "}\n",
    "            \n",
    "def check_target(unique):\n",
    "    target_formats = list(target.keys()) #we save the notation for each target format \n",
    "    return any(url.startswith(format) for format in target_formats) #checking whether each url is matched with our target urls under each notation \n",
    "\n",
    "for url in unique:\n",
    "    if check_target(unique):\n",
    "        interaction_function = target[url] #get the corresponding interaction function\n",
    "        interaction_function(driver, url)  \n",
    "driver.quit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c29a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "files = os.listdir(download_path)\n",
    "zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "print(zipfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "county = 'https://www2.census.gov/econ/bps/County/'\n",
    "driver.get(county)\n",
    "time.sleep(5)\n",
    "downloads = driver.find_elements(By.TAG_NAME, 'a')\n",
    "print(downloads)\n",
    "def process_txt_content(txt_content, output_csv_file):\n",
    "    lines = txt_content.split('\\n')\n",
    "    with open(output_csv_file, 'w') as csv_file:\n",
    "        for line in lines:\n",
    "            fields = line.strip().split()  # Adjust if needed\n",
    "            csv_line = ','.join(fields)\n",
    "            csv_file.write(csv_line + '\\n')\n",
    "            \n",
    "for download in downloads:\n",
    "    href = download.get_attribute('href')\n",
    "    if href:\n",
    "        files = href.split('/')[-1] #return the last element and save as files\n",
    "        if 'co' in files and files.endswith('.txt'):\n",
    "            download.click()\n",
    "            time.sleep(5)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'pre')))\n",
    "            txt_content_element = driver.find_element(By.TAG_NAME, 'pre')\n",
    "            txt_content = txt_content_element.text\n",
    "            process_txt_content(txt_content, files.replace('.txt', '.csv'))\n",
    "            driver.back()\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e103b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "import csv\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "base = 'https://www2.census.gov/econ/bps/County/'\n",
    "driver.get(base)\n",
    "time.sleep(5)\n",
    "def process_txt_content(txt_content, output_csv_file):\n",
    "    lines = txt_content.split('\\n')\n",
    "    with open(output_csv_file, 'w') as csv_file:\n",
    "        for line in lines:\n",
    "            fields = line.strip().split()  # Adjust if needed\n",
    "            csv_line = ','.join(fields)\n",
    "            csv_file.write(csv_line + '\\n')\n",
    "fourth_digit = 1\n",
    "third_digit = 8   #from year 2018 \n",
    "stop_loop = False #we define the stop loop for the entire iteration \n",
    "generated_filenames = set()\n",
    "while fourth_digit < 10 and not stop_loop:\n",
    "    for last_two_digits in range(1, 14):  # The last two digits are changing from 1 to 13\n",
    "        if fourth_digit == 2 and third_digit == 3 and last_two_digits == 7:\n",
    "            if file_type == 'c':\n",
    "                stop_loop = True\n",
    "                driver.quit()\n",
    "                break \n",
    "        if last_two_digits > 12:\n",
    "            third_digit += 1\n",
    "            if third_digit == 10:\n",
    "                fourth_digit += 1\n",
    "                third_digit = 0\n",
    "            adjusted_last_digits = last_two_digits - 12 \n",
    "            adjusted = True\n",
    "        else:\n",
    "            adjusted_last_digits = last_two_digits\n",
    "            adjusted = False  \n",
    "        for file_type in ['c', 'y']:\n",
    "            if fourth_digit == 2 and third_digit == 3 and last_two_digits == 7 and file_type == 'c':\n",
    "                stop_loop = True\n",
    "                driver.quit()\n",
    "                break\n",
    "            else:\n",
    "                file_name = f\"co{str(fourth_digit)}{str(third_digit)}{str(adjusted_last_digits).zfill(2)}{file_type}.txt\"\n",
    "                if file_name in generated_filenames:\n",
    "                    continue\n",
    "                generated_filenames.add(file_name) \n",
    "                full_url = base + file_name\n",
    "                driver.get(full_url)\n",
    "                driver.refresh()\n",
    "                time.sleep(2)\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'pre')))\n",
    "                txt_content_element = driver.find_element(By.TAG_NAME, 'pre')\n",
    "                txt_content = txt_content_element.text\n",
    "                process_txt_content(txt_content, file_name.replace('.txt', '.csv'))\n",
    "                time.sleep(2)\n",
    "                inputcsv_file = file_name.replace('.txt', '.csv') # we read the csv input based on txt extension label\n",
    "                table = pd.read_csv(inputcsv_file)\n",
    "                table['1-unit'] = table['1-unit'].astype(str)\n",
    "                table['County'][3:] = table['County'][3:] + ' ' + table['Unnamed: 6'][3:] + ' '+ table['1-unit'][3:].astype(str)     \n",
    "                table['County'] = table['County'].str.replace('nan', '')\n",
    "                table['Unnamed: 8'] = table['Unnamed: 8'].astype(str)\n",
    "                table['Unnamed: 9'] = table['Unnamed: 9'].astype(str)\n",
    "                table.loc[1:, 'Unnamed: 6'] = table['Unnamed: 8'][1:]\n",
    "                table.loc[1:, '1-unit'] = table['Unnamed: 9'][1:]\n",
    "                table.loc[1:, 'Unnamed: 9'] = table['2-unit'][1:]\n",
    "                table['Unnamed: 6'] = table['Unnamed: 6'].str.replace('nan', '0')\n",
    "                table['1-unit'] = table['1-unit'].str.replace('nan', '0')\n",
    "                outputcsv_file = inputcsv_file.replace('.csv', '_combined.csv')\n",
    "                table.to_csv(outputcsv_file, index=False)\n",
    "            if stop_loop:\n",
    "                break     \n",
    "        if adjusted:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d151cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_type in ['c', 'y']:\n",
    "            file_name = f\"co{str(fourth_digit)}{str(third_digit)}{str(adjusted_last_digits).zfill(2)}{file_type}.txt\"\n",
    "            full_url = base + file_name\n",
    "            if fourth_digit == 2 and third_digit == 3 and last_two_digits == 7 and file_type == 'c':\n",
    "                stop_loop = True \n",
    "                driver.quit()\n",
    "                break\n",
    "            driver.get(full_url)\n",
    "            driver.refresh()\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "import csv\n",
    "import numpy as np\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "base = 'https://www2.census.gov/econ/bps/County/'\n",
    "driver.get(base)\n",
    "time.sleep(5)\n",
    "def process_txt_content(txt_content, output_csv_file):\n",
    "    lines = txt_content.split('\\n')\n",
    "    with open(output_csv_file, 'w') as csv_file:\n",
    "        for line in lines:\n",
    "            fields = line.strip().split()  # Adjust if needed\n",
    "            csv_line = ','.join(fields)\n",
    "            csv_file.write(csv_line + '\\n')\n",
    "fourth_digit = 1\n",
    "third_digit = 0   #from year 2018 \n",
    "stop_loop = False #we define the stop loop for the entire iteration \n",
    "generated_filenames = set()\n",
    "while fourth_digit < 10 and not stop_loop:\n",
    "    for last_two_digits in range(1, 14):  # The last two digits are changing from 1 to 13\n",
    "        if fourth_digit == 2 and third_digit == 3 and last_two_digits == 7:\n",
    "            if file_type == 'c':\n",
    "                stop_loop = True\n",
    "                driver.quit()\n",
    "                break \n",
    "        if last_two_digits > 12:\n",
    "            third_digit += 1\n",
    "            if third_digit == 10:\n",
    "                fourth_digit += 1\n",
    "                third_digit = 0\n",
    "            adjusted_last_digits = last_two_digits - 12 \n",
    "            adjusted = True\n",
    "        else:\n",
    "            adjusted_last_digits = last_two_digits\n",
    "            adjusted = False  \n",
    "        for file_type in ['c', 'y']:\n",
    "            if fourth_digit == 2 and third_digit == 3 and last_two_digits == 7 and file_type == 'c':\n",
    "                stop_loop = True\n",
    "                driver.quit()\n",
    "                break\n",
    "            else:\n",
    "                file_name = f\"co{str(fourth_digit)}{str(third_digit)}{str(adjusted_last_digits).zfill(2)}{file_type}.txt\"\n",
    "                if file_name in generated_filenames:\n",
    "                    continue\n",
    "                generated_filenames.add(file_name) \n",
    "                full_url = base + file_name\n",
    "                driver.get(full_url)\n",
    "                driver.refresh()\n",
    "                time.sleep(2)\n",
    "                response = requests.get(full_url)\n",
    "                if response.status_code == 200:\n",
    "                    content = response.text\n",
    "                    local_file_path = f\"co{str(fourth_digit)}{str(third_digit)}{str(adjusted_last_digits).zfill(2)}{file_type}.txt\"\n",
    "                    with open(local_file_path, \"w\") as file:\n",
    "                        file.write(content)\n",
    "                    table = pd.read_csv(local_file_path, sep=',')\n",
    "                    table.reset_index(inplace=True)\n",
    "                    table.drop(columns=['FIPS.1', 'Region', 'Unnamed: 6', 'Unnamed: 9', 'Unnamed: 12', 'Unnamed: 15', 'Unnamed: 17', 'Unnamed: 18', '1-unit rep', 'Unnamed: 20', 'Unnamed: 21', '2-units rep', 'Unnamed: 23', 'Unnamed: 24', '3-4 units rep', 'Unnamed: 26', 'Unnamed: 27', ' 5+units rep'], inplace=True)\n",
    "                    table.rename(columns={'Survey': 'State', 'FIPS': 'County', 'Division': 'County Name', 'County': 'total1Unit', '1-unit': 'value1Unit', 'Unnamed: 8': 'total2Unit', '2-units': 'value2Unit', 'Unnamed: 11': 'total34Unit', '3-4 units': 'value34Unit', 'Unnamed: 14': 'total5Unit', '5+ units': 'value5Unit'}, inplace=True)\n",
    "                    table.drop(index=0, inplace=True)\n",
    "                    table['total1Unit'] = table['total1Unit'].astype(float)\n",
    "                    table['total2Unit'] = table['total2Unit'].astype(float)\n",
    "                    table['total34Unit'] = table['total34Unit'].astype(float)\n",
    "                    table['total5Unit'] = table['total5Unit'].astype(float)\n",
    "                    table['value1Unit'] = table['value1Unit'].astype(float)\n",
    "                    table['value2Unit'] = table['value2Unit'].astype(float)\n",
    "                    table['value34Unit'] = table['value34Unit'].astype(float)\n",
    "                    table['value5Unit'] = table['value5Unit'].astype(float)\n",
    "                    table['totalBuildings10'] = table['total1Unit'] + table['total2Unit'] + table['total34Unit'] + table['total5Unit']\n",
    "                    table['totalBuildingValue10'] = table['value1Unit'] + table['value2Unit'] + table['value34Unit'] + table['value5Unit']\n",
    "                    table['GEO_ID'] = \"0500000US\" + table['State'] + table['County']\n",
    "                    formatted_columns = ['State', 'County']  # Add more column names as needed\n",
    "                    for column in formatted_columns:\n",
    "                        if column == 'State':\n",
    "                            table[column] = table[column].apply(lambda x: f\"{int(x):02}\" if x.isdigit() else x)\n",
    "                        elif column == 'County':\n",
    "                            table[column] = table[column].apply(lambda x: f\"{int(x):03}\" if x.isdigit() else x)\n",
    "                    columns = ['index', 'State', 'County Name','County','totalBuildings10','totalBuildingValue10','GEO_ID']\n",
    "                    selected = table[columns]\n",
    "                    csv_file = f\"co{str(fourth_digit)}{str(third_digit)}{str(adjusted_last_digits).zfill(2)}{file_type}.csv\"\n",
    "                    selected.to_csv(csv_file, index=False)\n",
    "                    os.remove(local_file_path)\n",
    "                else:\n",
    "                    print(\"Failed to download the text file\")    \n",
    "                if stop_loop:\n",
    "                    break     \n",
    "            if adjusted:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = datetime.datetime.now().year  #get the current year\n",
    "for year in range(2007, current_year + 1): #we make for loop for each year \n",
    "    filtered_columns = [] #we define the empty filtered column \n",
    "    if year in [2009, 2013]: #because 2009 and 2013 overall ranking have unique html comparing to other \n",
    "        if year == 2009:\n",
    "            url = \"https://www.cnbc.com/id/100000992\"\n",
    "        elif year == 2013: #create additional condition\n",
    "            url = \"https://www.cnbc.com/id/100824779\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "    else: #other years \n",
    "        if year >= 2014:\n",
    "            for month in range(6,8):\n",
    "                for day in range(10,24):\n",
    "                    url = f\"https://www.cnbc.com/{year}/{month}/{day}/americas-top-states-for-business.html\"\n",
    "                    response = requests.get(url)\n",
    "                    if response.status_code == 200: #if that url is exisiting, code is 200\n",
    "                        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        table = soup.find('table')\n",
    "                        break\n",
    "                else:\n",
    "                    continue #creating iteration for checking the html \n",
    "                break \n",
    "        else:\n",
    "            url = f\"https://www.cnbc.com/top-states-{year}-overall-rankings/\" #the url linkfor overall rankings\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "    if table:\n",
    "        column_names = []\n",
    "        data_rows = []\n",
    "        for i, tr in enumerate(table.find_all('tr')):\n",
    "            if i == 0:  # Header row\n",
    "                th_tags = tr.find_all(['th', 'td'])\n",
    "                column_names = [th.text.strip() for th in th_tags if th.text.strip()]\n",
    "            else:\n",
    "                row = [td.text.strip() for td in tr.find_all('td')]\n",
    "                if row:\n",
    "                    data_rows.append(row)\n",
    "       \n",
    "        overall_labels = [\"Overall Rank\", \"Overall\",\"Rank\",\"Overall Ranking\"] #multiple labels\n",
    "        costofbusiness_labels = [\"Cost of Business\", \"Cost of Doing Business\"]\n",
    "        business_columns = [\"Business Friendliness\",\"Business Friendly\"]\n",
    "        Infra = [\"Infrastructure\",\"Transportation\",\"Infrastructure & Transp.\",\"Infrastructure &amp; Transp.\"] #does include transportation??\n",
    "        State = [\"State\"]\n",
    "        table_extract = pd.DataFrame(data_rows, columns=column_names)\n",
    "        table_extract.rename(columns=dict(zip(overall_labels, ['Overall Ranking']*len(overall_labels))), inplace=True)  # Rename into standard form\n",
    "        table_extract.rename(columns=dict(zip(costofbusiness_labels, ['Cost of Doing Business']*len(costofbusiness_labels))), inplace=True)  # Rename into standard form\n",
    "        table_extract.rename(columns=dict(zip(Infra, ['Infrastructure']*len(Infra))), inplace=True)  # Rename into standard form\n",
    "        table_extract.rename(columns=dict(zip(business_columns, ['Business Friendliness']*len(business_columns))), inplace=True)  # Rename into standard form\n",
    "        for label in overall_labels:\n",
    "            if label in table_extract.columns:\n",
    "                filtered_columns.insert(0, label)\n",
    "                    #rename into standard form \n",
    "                break\n",
    "        for label in costofbusiness_labels:\n",
    "            if label in table_extract.columns:\n",
    "                filtered_columns.insert(0, label)\n",
    "                break\n",
    "        for label in business_columns:\n",
    "            if label in table_extract.columns:\n",
    "                filtered_columns.insert(0, label)\n",
    "                break\n",
    "        for label in Infra:\n",
    "            if label in table_extract.columns:\n",
    "                filtered_columns.insert(0, label)\n",
    "                break\n",
    "        for label in State:\n",
    "            if label in table_extract.columns:\n",
    "                filtered_columns.insert(0, label)\n",
    "                break\n",
    "        filtered_data = table_extract[filtered_columns]\n",
    "        filtered_data.to_csv(f'extracted{year}.csv', index=False, header=True, mode='w')\n",
    "        print(f\" {year} downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f596dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "import csv\n",
    "import numpy as np\n",
    "table = pd.read_excel('VariableSources_2.xlsx')\n",
    "table\n",
    "interested_columns = table.iloc[:, [0, 2]]\n",
    "interested_columns\n",
    "grouped = table.groupby('URL')['Variable Name'].apply(list).reset_index()\n",
    "grouped.columns = ['URL', 'Variable Name']\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "import csv\n",
    "import numpy as np\n",
    "excelfile = 'complete-data-2021-umr-by-tti.xlsx'\n",
    "excel_extracted = pd.read_excel(excelfile)\n",
    "excel_extracted\n",
    "combined_labels = excel_extracted.iloc[:4].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=0) #combine first 4 row as one label \n",
    "excel_extracted = excel_extracted.iloc[4:] #replace first 4 row name\n",
    "excel_extracted.loc[-1] = combined_labels\n",
    "excel_extracted.index = excel_extracted.index + 1\n",
    "excel_extracted.sort_index(inplace=True)\n",
    "excel_extracted.rename(columns={'Unnamed: 23': 'Annual Hours of Delay'}, inplace=True) #renamed column name \n",
    "columns = ['Area Group', 'Urban Area', 'Primary','Population','Year','Arterial Street','Annual Hours of Delay']\n",
    "selected = excel_extracted[columns]\n",
    "csv_file_path = 'tmu.csv'\n",
    "selected.to_csv(csv_file_path, index=False)\n",
    "os.remove('complete-data-2021-umr-by-tti.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e711094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url_historical = 'https://econdata.s3-us-west-2.amazonaws.com/Reports/Core/RDC_Inventory_Core_Metrics_County_History.csv'\n",
    "driver.get(url_historical)\n",
    "time.sleep(70)\n",
    "table = pd.read_csv('RDC_Inventory_Core_Metrics_County_History.csv')\n",
    "table.rename(columns={'month_date_yyyymm': 'year&month'}, inplace=True)\n",
    "table.rename(columns={'county_name': 'County'}, inplace=True)\n",
    "table.rename(columns={'county_fips': 'County Federal Information Processing Standard'}, inplace=True)\n",
    "table.rename(columns={'median_square_feet': 'median listing square feet'}, inplace=True)\n",
    "columns = ['year&month', 'County', 'County Federal Information Processing Standard','median listing square feet']\n",
    "selected = table[columns]\n",
    "csv_file_path = 'realtor.csv'\n",
    "selected.to_csv(csv_file_path, index=False)\n",
    "os.remove('RDC_Inventory_Core_Metrics_County_History.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f77678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "table = pd.read_csv('RDC_Inventory_Core_Metrics_County_History.csv')\n",
    "table.rename(columns={'month_date_yyyymm': 'year&month'}, inplace=True)\n",
    "table.rename(columns={'county_name': 'County'}, inplace=True)\n",
    "table.rename(columns={'county_fips': 'County Federal Information Processing Standard'}, inplace=True)\n",
    "table.rename(columns={'median_square_feet': 'median listing square feet'}, inplace=True)\n",
    "columns = ['year&month', 'County', 'County Federal Information Processing Standard','median listing square feet']\n",
    "selected = table[columns]\n",
    "csv_file_path = 'realtor.csv'\n",
    "selected.to_csv(csv_file_path, index=False)\n",
    "os.remove('RDC_Inventory_Core_Metrics_County_History.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "table = pd.read_excel('bfs_county_apps_annual.xlsx', header= None)\n",
    "table = table.iloc[2:]\n",
    "selected = table\n",
    "csv_file_path = 'Business Applications by County.csv'\n",
    "selected.to_csv(csv_file_path, index=False)\n",
    "os.remove('bfs_county_apps_annual.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "def text_to_csv(input_filename, output_filename): #convert txt file to csv \n",
    "    with open(input_filename, 'r') as txt: #open the file\n",
    "        lines = txt.readlines() #read all the lines \n",
    "    with open(output_filename, 'w') as csv_file: #set up the csv file \n",
    "        for line in lines:\n",
    "            fields = line.strip().split()  # split the line\n",
    "            csv_line = ','.join(fields)    # Join fields with commas for CSV\n",
    "            csv_file.write(csv_line + '\\n')\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://taxfoundation.org/2023-state-business-tax-climate-index/' #the url for 2007 overall raking\n",
    "response = requests.get(url)  #get the webpage\n",
    "#require beautifulsoup to extract html documents\n",
    "Object = BeautifulSoup(response.content, 'html.parser')\n",
    "table = Object.find('table') #locate the table in the html\n",
    "column_names = []\n",
    "first_row = table.find('tr')\n",
    "for th in first_row.find_all(['th', 'td']):\n",
    "    column_name = th.text.strip()\n",
    "    if column_name:\n",
    "        column_names.append(column_name)\n",
    "data_rows = []  #we initalize the data row number \n",
    "for tr in table.find_all('tr'): #tr represent each number \n",
    "    row = [td.text.strip() for td in tr.find_all('td')] #we finds all the row elements within the current row element and create a new list containing the text content of each row element in the row\n",
    "    if row: #use to check whether there is empty row \n",
    "        data_rows.append(row) #if the row is not empty, then we show it in the data_row\n",
    "table_tax = pd.DataFrame(data_rows, columns=column_names)\n",
    "table_tax.to_csv('table_tax.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423781ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "table = pd.read_csv('bds2020_st_cty.csv')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "all_files = os.listdir(download_path)\n",
    "s0701 = [filename for filename in all_files if 'S0701' in filename] #contain all the lists that have s0701\n",
    "for filename in s0701:\n",
    "    if filename.endswith('.csv'):  # if it is csv file \n",
    "        file_path = os.path.join(download_path, filename)\n",
    "        table = pd.read_csv(file_path)\n",
    "        column_rename_mapping = {\n",
    "            'United States!!Moved; within same county!!Estimate': 'moved within same county, Estimation',\n",
    "            'United States!!Moved; within same county!!Margin of Error': 'moved within same county, Margin of Error',\n",
    "            'United States!!Moved; from different county, same state!!Estimate': 'moved within same state and same county, Estimation',\n",
    "            'United States!!Moved; from different county, same state!!Margin of Error': 'moved within same state and same county, Margin of Error',\n",
    "            'United States!!Moved; from different  state!!Estimate': 'moved within different state, Estimate',\n",
    "            'United States!!Moved; from different  state!!Margin of Error': 'moved within different state, Margin of Error',\n",
    "            'United States!!Moved; from abroad!!Estimate': 'moved abroad, Estimation',\n",
    "            'United States!!Moved; from abroad!!Margin of Error': 'moved abroad, Margin of Error',\n",
    "            'United States!!Total!!Estimate':'Total estimation',\n",
    "            'United States!!Total!!Margin of Error':'Total Margin of Error'\n",
    "        }\n",
    "        \n",
    "        table.rename(columns=column_rename_mapping, inplace=True)\n",
    "\n",
    "        table.to_csv(file_path, index=False)\n",
    "        print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "def text_to_csv(input_filename, output_filename): #convert txt file to csv \n",
    "    with open(input_filename, 'r') as txt: #open the file\n",
    "        lines = txt.readlines() #read all the lines \n",
    "    with open(output_filename, 'w') as csv_file: #set up the csv file \n",
    "        for line in lines:\n",
    "            fields = line.strip().split()  # split the line\n",
    "            csv_line = ','.join(fields)    # Join fields with commas for CSV\n",
    "            csv_file.write(csv_line + '\\n')\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://data.census.gov/table?q=geographic+mobility&tid=ACSST1Y2021.S0701'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "select.click()\n",
    "time.sleep(3)\n",
    "year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[2]/div[1]/div/div')\n",
    "year_5.click()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "geo.click()\n",
    "wait = WebDriverWait(driver,10)\n",
    "county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "county.click()\n",
    "time.sleep(5)\n",
    "Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "Clicklist.click()\n",
    "cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "cancel.click()\n",
    "wait = WebDriverWait(driver,15)\n",
    "download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "download.click()\n",
    "time.sleep(3)\n",
    "year_2020= driver.find_element(By.ID, 'Table: S0701 Vintage: 2020')\n",
    "year_2020.click()\n",
    "time.sleep(1)\n",
    "year_2019= driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div/div/div[2]/div[2]/div[2]/div/div/div[3]/div/div[5]/div/div')\n",
    "year_2019.click()\n",
    "time.sleep(1)\n",
    "elements_2018= driver.find_elements(By.ID, 'Table: S0701 Vintage: 2018') #there are two same ID \n",
    "if len(elements_2018) >= 2:\n",
    "    for element in elements_2018:\n",
    "        if element == elements_2018[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2017 = driver.find_elements(By.ID, 'Table: S0701 Vintage: 2017') #there are two same IDs\n",
    "if len(elements_2017) >= 2:\n",
    "    for Element in elements_2017:\n",
    "        if Element == elements_2017[1]:\n",
    "            Element.click()\n",
    "            break\n",
    "time.sleep(1)            \n",
    "download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "download_csv.click()\n",
    "time.sleep(10)\n",
    "attempts = 0\n",
    "max_attempts = 5  # Maximum number of attempts \n",
    "time_break = 10\n",
    "while attempts < max_attempts: #trying to do the mechanism \n",
    "    files = os.listdir(download_path)\n",
    "    zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "    if zipfiles:\n",
    "        for zip_file in zipfiles:\n",
    "            zip_file_path = os.path.join(download_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_path)\n",
    "            os.remove(zip_file_path)\n",
    "        break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "    else:\n",
    "        attempts += 1\n",
    "        if attempts < max_attempts:\n",
    "            print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "            time.sleep(time_break)\n",
    "        else:\n",
    "            print(\"No ZIP files found after maximum attempts.\")\n",
    "all_files = os.listdir(download_path)\n",
    "s0701 = [filename for filename in all_files if 'S0701' in filename] #contain all the lists that have s0701\n",
    "for filename in s0701:\n",
    "    if filename.endswith('.csv'):  # if it is csv file \n",
    "        file_path = os.path.join(download_path, filename)\n",
    "        table = pd.read_csv(file_path)\n",
    "        column_rename_mapping = {\n",
    "            'United States!!Moved; within same county!!Estimate': 'moved within same county, Estimation',\n",
    "            'United States!!Moved; within same county!!Margin of Error': 'moved within same county, Margin of Error',\n",
    "            'United States!!Moved; from different county, same state!!Estimate': 'moved within same state and same county, Estimation',\n",
    "            'United States!!Moved; from different county, same state!!Margin of Error': 'moved within same state and same county, Margin of Error',\n",
    "            'United States!!Moved; from different  state!!Estimate': 'moved within different state, Estimate',\n",
    "            'United States!!Moved; from different  state!!Margin of Error': 'moved within different state, Margin of Error',\n",
    "            'United States!!Moved; from abroad!!Estimate': 'moved abroad, Estimation',\n",
    "            'United States!!Moved; from abroad!!Margin of Error': 'moved abroad, Margin of Error',\n",
    "            'United States!!Total!!Estimate':'Total estimation',\n",
    "            'United States!!Total!!Margin of Error':'Total Margin of Error'\n",
    "        }\n",
    "        \n",
    "        table.rename(columns=column_rename_mapping, inplace=True)\n",
    "\n",
    "        table.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile  #the import file is zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime #find the current year and date \n",
    "years = range(2017, 2021)\n",
    "for year in years:\n",
    "    csv_file = f'ACSST5Y{year}.S0701-Data.csv'\n",
    "    csv_path = os.path.join(download_path, csv_file)\n",
    "    table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "    table.drop(0, axis=0, inplace=True)\n",
    "    table.columns = table.iloc[0]\n",
    "    table = table[1:]\n",
    "    columns_keyword = []\n",
    "    for column in table.columns:\n",
    "        if isinstance(column, str) and (\"!!Moved\" in column or \"Geo\" in column) and \"Annotation\" not in column:\n",
    "            columns_keyword.append(column)  \n",
    "    selected_columns = table[columns_keyword]\n",
    "    selected_columns.to_csv(f'{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67efa85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://hazards.fema.gov/nri/data-resources#csvDownload'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "download = driver.find_element(By.XPATH, '//*[@id=\"csvDownload\"]/div/div/div[1]/ul/li[1]/a')\n",
    "download.click()\n",
    "attempts = 0\n",
    "max_attempts = 5  # Maximum number of attempts \n",
    "time_break = 10\n",
    "while attempts < max_attempts: #trying to do the mechanism \n",
    "    files = os.listdir(download_path)\n",
    "    zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "    if zipfiles:\n",
    "        for zip_file in zipfiles:\n",
    "            zip_file_path = os.path.join(download_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_path)\n",
    "            os.remove(zip_file_path)\n",
    "        break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "    else:\n",
    "        attempts += 1\n",
    "        if attempts < max_attempts:\n",
    "            print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "            time.sleep(time_break)\n",
    "        else:\n",
    "            print(\"No ZIP files found after maximum attempts.\")\n",
    "files = os.listdir(download_path)\n",
    "for file in files:\n",
    "    if 'NRI' in file and not file.endswith('.csv'):\n",
    "        os.remove(os.path.join(download_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.page_load_strategy = 'eager'\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "desired_capabilities = DesiredCapabilities.CHROME.copy()\n",
    "desired_capabilities['goog:loggingPrefs'] = {'browser': 'ALL'}\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(\n",
    "    options=chrome_options\n",
    ")\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://hudgis-hud.opendata.arcgis.com/datasets/HUD::location-affordability-index-v-3-1/explore'\n",
    "driver.get(url)\n",
    "time.sleep(20)\n",
    "download = driver.find_element(By.CSS_SELECTOR, \"//*[@id='ember51']/div/div/div[3]/button\")\n",
    "download.click()\n",
    "download_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"ember51\"]/div/div/div[1]/div/div/div[3]/hub-download-card')))\n",
    "download_button.click()\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310249e",
   "metadata": {},
   "source": [
    "# DP02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://data.census.gov/table?+q=United+States&g=010XX00US&tid=ACSDP5Y2020.DP02'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "select.click()\n",
    "time.sleep(3)\n",
    "year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[4]/div[1]/div/div')\n",
    "year_5.click()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "geo.click()\n",
    "wait = WebDriverWait(driver,10)\n",
    "county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "county.click()\n",
    "time.sleep(5)\n",
    "Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "Clicklist.click()\n",
    "cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "cancel.click()\n",
    "wait = WebDriverWait(driver,15)\n",
    "download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "download.click()\n",
    "time.sleep(5)\n",
    "year_2020= driver.find_element(By.ID, 'Table: DP02 Vintage: 2020')\n",
    "year_2020.click()\n",
    "time.sleep(1)\n",
    "elements_2019= driver.find_elements(By.ID, 'Table: DP02 Vintage: 2019')\n",
    "if len(elements_2019) >= 2:\n",
    "    for element in elements_2019:\n",
    "        if element == elements_2019[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2018= driver.find_elements(By.ID, 'Table: DP02 Vintage: 2018') #there are two same ID \n",
    "if len(elements_2018) >= 2:\n",
    "    for element in elements_2018:\n",
    "        if element == elements_2018[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2017 = driver.find_elements(By.ID, 'Table: DP02 Vintage: 2017') #there are two same IDs\n",
    "if len(elements_2017) >= 2:\n",
    "    for Element in elements_2017:\n",
    "        if Element == elements_2017[1]:\n",
    "            Element.click()\n",
    "            break\n",
    "time.sleep(1)            \n",
    "download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "download_csv.click()\n",
    "time.sleep(120)\n",
    "attempts = 0\n",
    "max_attempts = 100  # Maximum number of attempts \n",
    "time_break = 10\n",
    "while attempts < max_attempts: #trying to do the mechanism \n",
    "    files = os.listdir(download_path)\n",
    "    zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "    if zipfiles:\n",
    "        for zip_file in zipfiles:\n",
    "            zip_file_path = os.path.join(download_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_path)\n",
    "            os.remove(zip_file_path)\n",
    "        break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "    else:\n",
    "        attempts += 1\n",
    "        if attempts < max_attempts:\n",
    "            print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "            time.sleep(time_break)\n",
    "        else:\n",
    "            print(\"No ZIP files found after maximum attempts.\")\n",
    "time.sleep(10)\n",
    "years = range(2017, 2022)\n",
    "for year in years:\n",
    "    csv_file = f'ACSDP5Y{year}.DP02-Data.csv' \n",
    "    csv_path = os.path.join(download_path, csv_file)\n",
    "    table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "    table.drop(0, axis=0, inplace=True)\n",
    "    table.columns = table.iloc[0]\n",
    "    table = table[1:]\n",
    "    columns_keyword = []\n",
    "    for column in table.columns:\n",
    "        if isinstance(column, str) and (\"Geo\" in column or \"total\" in column or \"Geo\" in column or \"fam\" in column or \"child\" in column or \"graduate\" in column or \"imm\" in column or \"bachelor\" in column) and (\"Annotation\" not in column and \"Margin\" not in column):\n",
    "            columns_keyword.append(column)  \n",
    "    selected_columns = table[columns_keyword]\n",
    "    selected_columns.to_csv(f'DP02-{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3dbbc",
   "metadata": {},
   "source": [
    "# S2301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://data.census.gov/table?q=unemployment&g=010XX00US&tid=ACSST1Y2021.S2301'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "select.click()\n",
    "time.sleep(3)\n",
    "year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[2]/div[1]/div/div')\n",
    "year_5.click()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "geo.click()\n",
    "wait = WebDriverWait(driver,10)\n",
    "county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "county.click()\n",
    "time.sleep(5)\n",
    "Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "Clicklist.click()\n",
    "cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "cancel.click()\n",
    "wait = WebDriverWait(driver,15)\n",
    "download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "download.click()\n",
    "time.sleep(5)\n",
    "year_2020= driver.find_element(By.ID, 'Table: S2301 Vintage: 2020')\n",
    "year_2020.click()\n",
    "time.sleep(1)\n",
    "elements_2019= driver.find_elements(By.ID, 'Table: S2301 Vintage: 2019')\n",
    "if len(elements_2019) >= 2:\n",
    "    for element in elements_2019:\n",
    "        if element == elements_2019[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2018= driver.find_elements(By.ID, 'Table: S2301 Vintage: 2018') #there are two same ID \n",
    "if len(elements_2018) >= 2:\n",
    "    for element in elements_2018:\n",
    "        if element == elements_2018[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2017 = driver.find_elements(By.ID, 'Table: S2301 Vintage: 2017') #there are two same IDs\n",
    "if len(elements_2017) >= 2:\n",
    "    for Element in elements_2017:\n",
    "        if Element == elements_2017[1]:\n",
    "            Element.click()\n",
    "            break\n",
    "time.sleep(1)            \n",
    "download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "download_csv.click()\n",
    "time.sleep(20)\n",
    "attempts = 0\n",
    "max_attempts = 100  # Maximum number of attempts \n",
    "time_break = 10\n",
    "while attempts < max_attempts: #trying to do the mechanism \n",
    "    files = os.listdir(download_path)\n",
    "    zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "    if zipfiles:\n",
    "        for zip_file in zipfiles:\n",
    "            zip_file_path = os.path.join(download_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_path)\n",
    "            os.remove(zip_file_path)\n",
    "        break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "    else:\n",
    "        attempts += 1\n",
    "        if attempts < max_attempts:\n",
    "            print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "            time.sleep(time_break)\n",
    "        else:\n",
    "            print(\"No ZIP files found after maximum attempts.\")\n",
    "time.sleep(10)\n",
    "years = range(2017, 2022)\n",
    "for year in years:\n",
    "    csv_file = f'ACSST5Y{year}.S2301-Data.csv' \n",
    "    csv_path = os.path.join(download_path, csv_file)\n",
    "    table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "    table.drop(0, axis=0, inplace=True)\n",
    "    table.columns = table.iloc[0]\n",
    "    table = table[1:]\n",
    "    columns_keyword = []\n",
    "    for column in table.columns:\n",
    "        if isinstance(column, str) and (\"Geo\" in column or \"total\" in column or \"Geo\" in column or \"Employ\" in column or \"Unemploy\" in column) and (\"Annotation\" not in column and \"Margin\" not in column):\n",
    "            columns_keyword.append(column)  \n",
    "    selected_columns = table[columns_keyword]\n",
    "    selected_columns.to_csv(f'S2301-{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80053890",
   "metadata": {},
   "source": [
    "# B25104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e433806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://data.census.gov/table?q=monthly+housing+payment&tid=ACSDT1Y2021.B25104'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "select.click()\n",
    "time.sleep(3)\n",
    "year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[2]/div[1]/div/div')\n",
    "year_5.click()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "geo.click()\n",
    "wait = WebDriverWait(driver,10)\n",
    "county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "county.click()\n",
    "time.sleep(5)\n",
    "Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "Clicklist.click()\n",
    "cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "cancel.click()\n",
    "wait = WebDriverWait(driver,15)\n",
    "download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "download.click()\n",
    "time.sleep(5)\n",
    "year_2020= driver.find_element(By.ID, 'Table: B25104 Vintage: 2020')\n",
    "year_2020.click()\n",
    "time.sleep(1)\n",
    "elements_2019= driver.find_elements(By.ID, 'Table: B25104 Vintage: 2019')\n",
    "if len(elements_2019) >= 2:\n",
    "    for element in elements_2019:\n",
    "        if element == elements_2019[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2018= driver.find_elements(By.ID, 'Table: B25104 Vintage: 2018') #there are two same ID \n",
    "if len(elements_2018) >= 2:\n",
    "    for element in elements_2018:\n",
    "        if element == elements_2018[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2017 = driver.find_elements(By.ID, 'Table: B25104 Vintage: 2017') #there are two same IDs\n",
    "if len(elements_2017) >= 2:\n",
    "    for Element in elements_2017:\n",
    "        if Element == elements_2017[1]:\n",
    "            Element.click()\n",
    "            break\n",
    "time.sleep(1)            \n",
    "download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "download_csv.click()\n",
    "time.sleep(20)\n",
    "attempts = 0\n",
    "max_attempts = 100  # Maximum number of attempts \n",
    "time_break = 10\n",
    "while attempts < max_attempts: #trying to do the mechanism \n",
    "    files = os.listdir(download_path)\n",
    "    zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "    if zipfiles:\n",
    "        for zip_file in zipfiles:\n",
    "            zip_file_path = os.path.join(download_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_path)\n",
    "            os.remove(zip_file_path)\n",
    "        break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "    else:\n",
    "        attempts += 1\n",
    "        if attempts < max_attempts:\n",
    "            print(f\"No ZIP files found in the download_path. wait {time_break}second for retrying...\")\n",
    "            time.sleep(time_break)\n",
    "        else:\n",
    "            print(\"No ZIP files found after maximum attempts.\")\n",
    "time.sleep(10)\n",
    "years = range(2017, 2022)\n",
    "for year in years:\n",
    "    csv_file = f'ACSDT5Y{year}.B25104-Data.csv' \n",
    "    csv_path = os.path.join(download_path, csv_file)\n",
    "    table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "    table.drop(0, axis=0, inplace=True)\n",
    "    table.columns = table.iloc[0]\n",
    "    table = table[1:]\n",
    "    columns_keyword = []\n",
    "    for column in table.columns:\n",
    "        if isinstance(column, str) and (\"Geo\" in column or \"Total\" in column) and (\"Annotation\" not in column and \"Margin\" not in column):\n",
    "            columns_keyword.append(column)  \n",
    "    selected_columns = table[columns_keyword]\n",
    "    selected_columns.to_csv(f'B25104-{year}.csv', index=False)\n",
    "    os.remove(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad68713",
   "metadata": {},
   "source": [
    "# S0802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://data.census.gov/table?q=commute&g=010XX00US&tid=ACSST1Y2021.S0802'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "select.click()\n",
    "time.sleep(3)\n",
    "year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[2]/div[1]/div/div')\n",
    "year_5.click()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "geo.click()\n",
    "wait = WebDriverWait(driver,10)\n",
    "county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "county.click()\n",
    "time.sleep(5)\n",
    "Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "Clicklist.click()\n",
    "cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "cancel.click()\n",
    "wait = WebDriverWait(driver,15)\n",
    "download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "download.click()\n",
    "time.sleep(5)\n",
    "year_2020= driver.find_element(By.ID, 'Table: S0802 Vintage: 2020')\n",
    "year_2020.click()\n",
    "time.sleep(1)\n",
    "elements_2019= driver.find_elements(By.ID, 'Table: S0802 Vintage: 2019')\n",
    "if len(elements_2019) >= 2:\n",
    "    for element in elements_2019:\n",
    "        if element == elements_2019[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2018= driver.find_elements(By.ID, 'Table: S0802 Vintage: 2018') #there are two same ID \n",
    "if len(elements_2018) >= 2:\n",
    "    for element in elements_2018:\n",
    "        if element == elements_2018[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2017 = driver.find_elements(By.ID, 'Table: S0802 Vintage: 2017') #there are two same IDs\n",
    "if len(elements_2017) >= 2:\n",
    "    for Element in elements_2017:\n",
    "        if Element == elements_2017[1]:\n",
    "            Element.click()\n",
    "            break\n",
    "time.sleep(1)            \n",
    "download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "download_csv.click()\n",
    "time.sleep(10)\n",
    "attempts = 0\n",
    "max_attempts = 100  # Maximum number of attempts \n",
    "time_break = 10\n",
    "while attempts < max_attempts: #trying to do the mechanism \n",
    "    files = os.listdir(download_path)\n",
    "    zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "    if zipfiles:\n",
    "        for zip_file in zipfiles:\n",
    "            zip_file_path = os.path.join(download_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_path)\n",
    "            os.remove(zip_file_path)\n",
    "        break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "    else:\n",
    "        attempts += 1\n",
    "        if attempts < max_attempts:\n",
    "            time.sleep(time_break)\n",
    "        else:\n",
    "            print(\"No ZIP files found after maximum attempts.\")\n",
    "years = range(2017, 2022)\n",
    "for year in years:\n",
    "    csv_file = f'ACSST5Y{year}.S0802-Data.csv' \n",
    "    csv_path = os.path.join(download_path, csv_file)\n",
    "    table = pd.read_csv(csv_path, header=None, dtype=str)\n",
    "    table.drop(0, axis=0, inplace=True)\n",
    "    table.columns = table.iloc[0]\n",
    "    table = table[1:]\n",
    "    columns_keyword = []\n",
    "    for column in table.columns:\n",
    "        if isinstance(column, str) and (\"Geo\" in column or \"TRAVEL TIME \" in column or \"Median earnings\" in column) and (\"Annotation\" not in column and \"Margin\" not in column):\n",
    "            columns_keyword.append(column)  \n",
    "    selected_columns = table[columns_keyword]\n",
    "    selected_columns.to_csv(f'S0802-{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997566e",
   "metadata": {},
   "source": [
    "# B14004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://data.census.gov/table?q=B14004:+SEX+BY+COLLEGE+OR+GRADUATE+SCHOOL+ENROLLMENT+BY+TYPE+OF+SCHOOL+BY+AGE+FOR+THE+POPULATION+15+YEARS+AND+OVER&tid=ACSDT1Y2021.B14004'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "select = driver.find_element(By.ID, 'toolbar-button-datasets')\n",
    "select.click()\n",
    "time.sleep(3)\n",
    "year_5 = driver.find_element(By.XPATH, '//*[@id=\"table-header\"]/div[2]/div[2]/div/div/div/div[2]/ul/div/li[2]/div[1]')\n",
    "year_5.click()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "geo = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"toolbar-button-geos\"]')))\n",
    "geo.click()\n",
    "wait = WebDriverWait(driver,10)\n",
    "county = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/div[2]/div[2]/div/div[3]')))\n",
    "county.click()\n",
    "time.sleep(5)\n",
    "Clicklist = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[2]/div/div/div/section/ul/li[1]/div/div[1]/div[2]')\n",
    "Clicklist.click()\n",
    "cancel = driver.find_element(By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[2]/div/div/div[1]/div/div[1]/div/div[1]/div[1]/div/div[4]')\n",
    "cancel.click()\n",
    "wait = WebDriverWait(driver,15)\n",
    "download = wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"content-layout\"]/div[2]/div[1]/div/div[4]/main/div[1]/div[2]/div[2]/div/div/div[4]/div/button')))\n",
    "download.click()\n",
    "time.sleep(5)\n",
    "year_2020= driver.find_element(By.ID, 'Table: B14004 Vintage: 2020')\n",
    "year_2020.click()\n",
    "time.sleep(1)\n",
    "elements_2019= driver.find_elements(By.ID, 'Table: B14004 Vintage: 2019')\n",
    "if len(elements_2019) >= 2:\n",
    "    for element in elements_2019:\n",
    "        if element == elements_2019[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2018= driver.find_elements(By.ID, 'Table: B14004 Vintage: 2018') #there are two same ID \n",
    "if len(elements_2018) >= 2:\n",
    "    for element in elements_2018:\n",
    "        if element == elements_2018[1]:\n",
    "            element.click()\n",
    "            break\n",
    "time.sleep(1)\n",
    "elements_2017 = driver.find_elements(By.ID, 'Table: B14004 Vintage: 2017') #there are two same IDs\n",
    "if len(elements_2017) >= 2:\n",
    "    for Element in elements_2017:\n",
    "        if Element == elements_2017[1]:\n",
    "            Element.click()\n",
    "            break\n",
    "time.sleep(1)            \n",
    "download_csv = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[3]/div/div/div[2]/div[2]/div[4]/div[2]/button')\n",
    "download_csv.click()\n",
    "time.sleep(120)\n",
    "time.sleep(10)\n",
    "attempts = 0\n",
    "max_attempts = 100  # Maximum number of attempts \n",
    "time_break = 10\n",
    "while attempts < max_attempts: #trying to do the mechanism \n",
    "    files = os.listdir(download_path)\n",
    "    zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "    if zipfiles:\n",
    "        for zip_file in zipfiles:\n",
    "            zip_file_path = os.path.join(download_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_path)\n",
    "            os.remove(zip_file_path)\n",
    "        break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "    else:\n",
    "        attempts += 1\n",
    "        if attempts < max_attempts:\n",
    "            time.sleep(time_break)\n",
    "        else:\n",
    "            print(\"No ZIP files found after maximum attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000542fb",
   "metadata": {},
   "source": [
    "# libraray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "url = 'https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey'\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "driver.execute_script(\"window.scrollBy(0, 1000);\") \n",
    "time.sleep(3)\n",
    "year_2021 = driver.find_element(By.ID, 'fy-2021')\n",
    "year_2021.click()\n",
    "time.sleep(1)\n",
    "download = driver.find_element(By.XPATH, '//*[@id=\"ui-id-1\"]/ul/li[1]/a[1]')\n",
    "download.click()\n",
    "time.sleep(5)\n",
    "attempts = 0\n",
    "max_attempts = 100  # Maximum number of attempts \n",
    "time_break = 10\n",
    "while attempts < max_attempts: #trying to do the mechanism \n",
    "    files = os.listdir(download_path)\n",
    "    zipfiles = [file for file in files if file.endswith('.zip')]\n",
    "    if zipfiles:\n",
    "        for zip_file in zipfiles:\n",
    "            zip_file_path = os.path.join(download_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(download_path)\n",
    "            os.remove(zip_file_path)\n",
    "        break  # Break out of the retry loop if zip files are found and extracted successfully\n",
    "    else:\n",
    "        attempts += 1\n",
    "        if attempts < max_attempts:\n",
    "            time.sleep(time_break)\n",
    "        else:\n",
    "            print(\"No ZIP files found after maximum attempts.\")\n",
    "time.sleep(1)\n",
    "extracted_folder = \"/Users/frrookie521/Desktop/civil/cee495/PLS_FY2021 PUD_CSV/\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "csv_files = [\"PLS_FY21_AE_pud21i.csv\", \"pls_fy21_outlet_pud21i.csv\"]\n",
    "for csv_filename in csv_files:\n",
    "    extracted_file_path = os.path.join(extracted_folder, csv_filename)\n",
    "    download_file_path = os.path.join(download_path, csv_filename)\n",
    "    try:\n",
    "        shutil.move(extracted_file_path, download_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error extracting the file '{csv_filename}'\")   \n",
    "shutil.rmtree(extracted_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121356c",
   "metadata": {},
   "source": [
    "# Interstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "PATH = \"/Users/frrookie521/Desktop/civil/cee495/cee495/Google\\ Chrome\\ for\\ Testing.app\"\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "prefs = {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "         }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "excel_file = pd.read_excel('VariableSources_2.xlsx', header=None)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_window_size(1396, 1000)\n",
    "urls = excel_file.iloc[1:, 2].tolist() #store urls from third column and second row \n",
    "unique = set(url for url in urls if isinstance(url, str)) #extact the unique urls from excel\n",
    "zipfiles = ['file1.zip', 'file2.zip'] \n",
    "years = range(2015, 2022)\n",
    "download_xpath_years = {\n",
    "    2015: '//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a',\n",
    "    2016: '//*[@id=\"fullpage\"]/table[4]/tbody/tr[4]/td[3]/a',\n",
    "    2017: '//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a',\n",
    "    2018: '//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a',\n",
    "    2019: '//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a',\n",
    "    2020: '//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a',\n",
    "    2021: '//*[@id=\"fullpage\"]/table[3]/tbody/tr[5]/td[3]/a'\n",
    "}\n",
    "\n",
    "for year in years:\n",
    "    html = f'https://www.fhwa.dot.gov/policyinformation/statistics/{year}/index.cfm#sec4' \n",
    "    driver.get(html)\n",
    "    time.sleep(1)\n",
    "    download_xpath = download_xpath_years.get(year) #select the correct xpath based on year     \n",
    "    if download_xpath:\n",
    "        download = driver.find_element(By.XPATH, download_xpath)\n",
    "        download.click()\n",
    "        time.sleep(2)\n",
    "        old_excel = 'vmt422c.xls'\n",
    "        if year == 2021:\n",
    "            old_excel = 'hm220.xls'\n",
    "        new_excel = f\"{year}_interstate.xls\"\n",
    "        os.rename(old_excel, new_excel)\n",
    "    else:\n",
    "        print(f\"XPath for {year} not found.\")\n",
    "table = pd.read_excel('2021_interstate.xls')\n",
    "rows_to_search = [6, 7]\n",
    "filtered_columns = []\n",
    "keywords = ['Interstate', 'Total', 'Year']\n",
    "for row in rows_to_search:\n",
    "        filtered_columns += [col for col in table.columns if any(keyword.lower() in str(table.iloc[row][col]).lower() for keyword in keywords)]\n",
    "new_table = table[filtered_columns]\n",
    "new_table.reset_index(drop=True, inplace=True)\n",
    "new_table = new_table.iloc[6:]\n",
    "new_table = new_table.drop(8)\n",
    "new_table.reset_index(drop=True, inplace=True)\n",
    "new_table.iat[1, 0] = 'YEAR'\n",
    "new_table.iat[1, 1] = 'Total rural and urban (Miles)'\n",
    "new_table.iat[1, 2] = 'Rural interstate (Miles)'\n",
    "new_table.iat[1, 3] = 'Total in Rural (Miles)'\n",
    "new_table.iat[1, 4] = 'Urban interstate (Miles)'\n",
    "new_table.iat[1, 5] = 'Urban total (Miles)'\n",
    "new_table = new_table.drop(0)\n",
    "new_table.reset_index(drop=True, inplace=True)\n",
    "new_table.columns = new_table.iloc[0]\n",
    "new_table = new_table.iloc[1:].reset_index(drop=True)\n",
    "new_table.to_csv('1980 to 2021_highway.csv', index=False)\n",
    "time.sleep(4)\n",
    "os.remove('2021_interstate.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# Create an ActionChains instance\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "# JavaScript code to capture click coordinates and send them back to Python\n",
    "click_capture_script = \"\"\"\n",
    "document.addEventListener('click', function(event) {\n",
    "    var x = event.clientX;\n",
    "    var y = event.clientY;\n",
    "    console.log('Clicked at X:', x, 'Y:', y);\n",
    "\n",
    "    // Send the coordinates back to Python\n",
    "    window.clickCoordinates = { x: x, y: y };\n",
    "});\n",
    "\"\"\"\n",
    "\n",
    "# Execute the JavaScript code in the browser\n",
    "driver.execute_script(click_capture_script)\n",
    "\n",
    "# Perform a manual click on the webpage (you can also use Selenium to click on an element within the viewport)\n",
    "# Example: driver.find_element(By.XPATH, 'your_element_xpath').click()\n",
    "\n",
    "# Wait for some time to allow for clicks and logging\n",
    "input('Press Enter to quit...')\n",
    "\n",
    "# Retrieve the coordinates from JavaScript and print them in Python\n",
    "click_coordinates = driver.execute_script('return window.clickCoordinates;')\n",
    "print('Clicked at X:', click_coordinates['x'], 'Y:', click_coordinates['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af0039",
   "metadata": {},
   "source": [
    "//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a 2015\n",
    "//*[@id=\"fullpage\"]/table[4]/tbody/tr[4]/td[3]/a 2016 \n",
    "//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a 2017\n",
    "//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a 2018\n",
    "//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a 2019\n",
    "//*[@id=\"fullpage\"]/table[3]/tbody/tr[4]/td[3]/a 2020\n",
    "//*[@id=\"fullpage\"]/table[3]/tbody/tr[5]/td[3]/a 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "table = pd.read_excel('2015_interstate.xls')\n",
    "keywords = ['year', 'Year']\n",
    "lowercase_table = table.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "mask = lowercase_table.isin(keywords)\n",
    "row_number = mask.any(axis=1).idxmax()\n",
    "new_table = table.iloc[row_number:]\n",
    "new_table.reset_index(drop=True, inplace=True)\n",
    "new_table = new_table.drop(table.columns[0], axis=1)\n",
    "new_table.columns = new_table.iloc[0]\n",
    "new_table = new_table.iloc[1:]\n",
    "new_table = new_table[new_table.iloc[:, 2] >= 1980]\n",
    "new_table.reset_index(drop=True, inplace=True)\n",
    "new_table.rename(columns={new_table.columns[3]: 'VMT Millions'}, inplace=True)\n",
    "new_table.rename(columns={new_table.columns[4]: 'VMT Actural'}, inplace=True)\n",
    "new_table.rename(columns={new_table.columns[1]: 'Public road Millage'}, inplace=True)\n",
    "new_table.to_csv('2015_interstate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "table = pd.read_excel('2021_interstate.xls')\n",
    "rows_to_search = [6, 7]\n",
    "filtered_columns = []\n",
    "keywords = ['Interstate', 'Total', 'Year']\n",
    "for row in rows_to_search:\n",
    "        filtered_columns += [col for col in table.columns if any(keyword.lower() in str(table.iloc[row][col]).lower() for keyword in keywords)]\n",
    "new_table = table[filtered_columns]\n",
    "new_table.reset_index(drop=True, inplace=True)\n",
    "new_table = new_table.iloc[6:]\n",
    "new_table = new_table.drop(8)\n",
    "new_table.reset_index(drop=True, inplace=True)\n",
    "new_table.iat[1, 0] = 'YEAR'\n",
    "new_table.iat[1, 1] = 'Total rural and urban (Miles)'\n",
    "new_table.iat[1, 2] = 'Rural interstate (Miles)'\n",
    "new_table.iat[1, 3] = 'Total in Rural (Miles)'\n",
    "new_table.iat[1, 4] = 'Urban interstate (Miles)'\n",
    "new_table.iat[1, 5] = 'Urban total (Miles)'\n",
    "new_table = new_table.drop(0)\n",
    "new_table.reset_index(drop=True, inplace=True)\n",
    "new_table.columns = new_table.iloc[0]\n",
    "new_table = new_table.iloc[1:].reset_index(drop=True)\n",
    "new_table.to_csv('1980 to 2021_interstate.csv', index=False)\n",
    "time.sleep(4)\n",
    "os.remove('2021_interstate.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "table = pd.read_excel('2017_interstate.xls')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton\n",
    "import sys\n",
    "import re\n",
    "import zipfile #extract zip file\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "download_path = \"/Users/frrookie521/Desktop/civil/cee495/\"\n",
    "years = range(2015, 2022)\n",
    "for year in years:\n",
    "    excel_file = os.path.join(download_path, f\"{year}_interstate.xls\")\n",
    "    csv_file = os.path.join(download_path, f\"{year}_interstate.csv\" )\n",
    "    table = pd.read_excel(excel_file, engine='xlrd')     \n",
    "    keywords = ['year', 'Year']\n",
    "    lowercase_table = table.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    mask = lowercase_table.isin(keywords) #create true or false for the keyword row \n",
    "    row_number = mask.any(axis=1).idxmax()\n",
    "    new_table = table.iloc[row_number:]\n",
    "    new_table.reset_index(drop=True, inplace=True)\n",
    "    new_table = new_table.drop(table.columns[0], axis=1)\n",
    "    new_table.columns = new_table.iloc[0]\n",
    "    new_table = new_table.iloc[1:]\n",
    "    new_table.iloc[:, 2] = pd.to_numeric(new_table.iloc[:, 2], errors='coerce') \n",
    "    new_table = new_table[new_table.iloc[:, 2] >= 1980]\n",
    "    new_table.reset_index(drop=True, inplace=True)\n",
    "    new_table.rename(columns={new_table.columns[3]: 'VMT Millions'}, inplace=True)\n",
    "    new_table.rename(columns={new_table.columns[4]: 'VMT Actural'}, inplace=True)\n",
    "    new_table.rename(columns={new_table.columns[1]: 'Public road Millage'}, inplace=True)\n",
    "    new_table.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a99509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
